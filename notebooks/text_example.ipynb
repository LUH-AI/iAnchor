{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import spacy\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from Anchor.anchor import Anchor, Tasktype\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset from http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "# Link: http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
    "def load_polarity(path='/home/kevin/iml-ws21-projects-risingnumpygods/datasets/rt-polaritydata/'):\n",
    "    data = []\n",
    "    labels = []\n",
    "    f_names = ['rt-polarity.neg', 'rt-polarity.pos']\n",
    "    for (l, f) in enumerate(f_names):\n",
    "        for line in open(os.path.join(path, f), 'rb'):\n",
    "            try:\n",
    "                line.decode('utf8')\n",
    "            except:\n",
    "                continue\n",
    "            data.append(line.strip())\n",
    "            labels.append(l)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = load_polarity()\n",
    "train, test, train_labels, test_labels = sklearn.model_selection.train_test_split(data, labels, test_size=.2, random_state=42)\n",
    "train, val, train_labels, val_labels = sklearn.model_selection.train_test_split(train, train_labels, test_size=.1, random_state=42)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer.fit(train)\n",
    "train_vectors = vectorizer.transform(train)\n",
    "test_vectors = vectorizer.transform(test)\n",
    "val_vectors = vectorizer.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy 0.7544910179640718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/iML/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "c = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "c.fit(train_vectors, train_labels)\n",
    "\n",
    "preds = c.predict(val_vectors)\n",
    "\n",
    "print('Val accuracy', sklearn.metrics.accuracy_score(val_labels, preds))\n",
    "\n",
    "def predict_lr(texts):\n",
    "    return c.predict(vectorizer.transform(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: Start Sampling\n",
      "INFO:root: Start Greedy Search\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kevin/iml-ws21-projects-risingnumpygods/notebooks/text_example.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kevin/iml-ws21-projects-risingnumpygods/notebooks/text_example.ipynb#ch0000004vscode-remote?line=5'>6</a>\u001b[0m explainer \u001b[39m=\u001b[39m explainer \u001b[39m=\u001b[39m Anchor(Tasktype\u001b[39m.\u001b[39mTEXT)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kevin/iml-ws21-projects-risingnumpygods/notebooks/text_example.ipynb#ch0000004vscode-remote?line=7'>8</a>\u001b[0m method_paras \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdesired_confidence\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.8\u001b[39m}\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kevin/iml-ws21-projects-risingnumpygods/notebooks/text_example.ipynb#ch0000004vscode-remote?line=8'>9</a>\u001b[0m anchor \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mexplain_instance(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kevin/iml-ws21-projects-risingnumpygods/notebooks/text_example.ipynb#ch0000004vscode-remote?line=9'>10</a>\u001b[0m     preprocessed_text,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kevin/iml-ws21-projects-risingnumpygods/notebooks/text_example.ipynb#ch0000004vscode-remote?line=10'>11</a>\u001b[0m     predict_lr,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kevin/iml-ws21-projects-risingnumpygods/notebooks/text_example.ipynb#ch0000004vscode-remote?line=11'>12</a>\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgreedy\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kevin/iml-ws21-projects-risingnumpygods/notebooks/text_example.ipynb#ch0000004vscode-remote?line=12'>13</a>\u001b[0m     method_specific\u001b[39m=\u001b[39;49mmethod_paras,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kevin/iml-ws21-projects-risingnumpygods/notebooks/text_example.ipynb#ch0000004vscode-remote?line=13'>14</a>\u001b[0m     num_coverage_samples\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kevin/iml-ws21-projects-risingnumpygods/notebooks/text_example.ipynb#ch0000004vscode-remote?line=14'>15</a>\u001b[0m )\n",
      "File \u001b[0;32m~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py:97\u001b[0m, in \u001b[0;36mAnchor.explain_instance\u001b[0;34m(self, input, predict_fn, method, task_specific, method_specific, num_coverage_samples, epsilon, batch_size, verbose, seed)\u001b[0m\n\u001b[1;32m     <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=94'>95</a>\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgreedy\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=95'>96</a>\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m Start Greedy Search\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=96'>97</a>\u001b[0m     exp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__greedy_anchor(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmethod_specific)\n\u001b[1;32m     <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=97'>98</a>\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbeam\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=98'>99</a>\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m Start Beam Search\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py:202\u001b[0m, in \u001b[0;36mAnchor.__greedy_anchor\u001b[0;34m(self, desired_confidence, min_coverage)\u001b[0m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=197'>198</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=198'>199</a>\u001b[0m \u001b[39mGreedy Approach to calculate the shortest anchor, which fullfills the precision constraint EQ3.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=199'>200</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=200'>201</a>\u001b[0m candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_candidates([], min_coverage)\n\u001b[0;32m--> <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=201'>202</a>\u001b[0m anchor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkl_lucb\u001b[39m.\u001b[39;49mget_best_candidates(candidates, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msampler, \u001b[39m1\u001b[39;49m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=203'>204</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__check_valid_candidate(\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=204'>205</a>\u001b[0m     anchor, \u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, desired_confidence\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=205'>206</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/anchor.py?line=206'>207</a>\u001b[0m     candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_candidates([anchor], min_coverage)\n",
      "File \u001b[0;32m~/iml-ws21-projects-risingnumpygods/Anchor/bandit.py:54\u001b[0m, in \u001b[0;36mKL_LUCB.get_best_candidates\u001b[0;34m(self, candidates, sampler, top_n)\u001b[0m\n\u001b[1;32m     <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/bandit.py?line=51'>52</a>\u001b[0m \u001b[39mwhile\u001b[39;00m prec_diff \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps:\n\u001b[1;32m     <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/bandit.py?line=52'>53</a>\u001b[0m     candidates[ut], _, _ \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39msample(candidates[ut], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m---> <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/bandit.py?line=53'>54</a>\u001b[0m     candidates[lt], _, _ \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39;49msample(candidates[lt], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size)\n\u001b[1;32m     <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/bandit.py?line=55'>56</a>\u001b[0m     t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/bandit.py?line=56'>57</a>\u001b[0m     lt, ut, prec_lb, prec_ub \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__update_bounds(\n\u001b[1;32m     <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/bandit.py?line=57'>58</a>\u001b[0m         candidates, prec_lb, prec_ub, t, top_n\n\u001b[1;32m     <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/bandit.py?line=58'>59</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py:499\u001b[0m, in \u001b[0;36mTextSampler.sample\u001b[0;34m(self, candidate, num_samples, calculate_labels)\u001b[0m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=495'>496</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m calculate_labels:\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=496'>497</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, feature_masks, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=498'>499</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__sample_pertubated_sentences(candidate, feature_masks, num_samples)\n",
      "File \u001b[0;32m~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py:551\u001b[0m, in \u001b[0;36mTextSampler.__sample_pertubated_sentences\u001b[0;34m(self, candidate, data, num_samples)\u001b[0m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=533'>534</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__sample_pertubated_sentences\u001b[39m(\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=534'>535</a>\u001b[0m     \u001b[39mself\u001b[39m, candidate: AnchorCandidate, data: np\u001b[39m.\u001b[39mndarray, num_samples: \u001b[39mint\u001b[39m,\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=535'>536</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[AnchorCandidate, np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mndarray]:\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=536'>537</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=537'>538</a>\u001b[0m \u001b[39m    Generate num_sampels new sentences (via self.__generate_sentence),\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=538'>539</a>\u001b[0m \u001b[39m    predicts the labels and updates the precision for the AnchorCandidate\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=548'>549</a>\u001b[0m \u001b[39m        Tuple[AnchorCandidate, np.ndarray, np.ndarray]: Structure [AnchorCandidate, feature_masks, None]\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=549'>550</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=550'>551</a>\u001b[0m     sentences \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mapply_along_axis(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__generate_sentence, \u001b[39m1\u001b[39;49m, data)\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=551'>552</a>\u001b[0m         \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=552'>553</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=554'>555</a>\u001b[0m     \u001b[39m# predict pertubed sentences\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=555'>556</a>\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_fn(sentences\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/numpy/lib/shape_base.py:402\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/numpy/lib/shape_base.py?line=399'>400</a>\u001b[0m buff[ind0] \u001b[39m=\u001b[39m res\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/numpy/lib/shape_base.py?line=400'>401</a>\u001b[0m \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m inds:\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/numpy/lib/shape_base.py?line=401'>402</a>\u001b[0m     buff[ind] \u001b[39m=\u001b[39m asanyarray(func1d(inarr_view[ind], \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/numpy/lib/shape_base.py?line=403'>404</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(res, matrix):\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/numpy/lib/shape_base.py?line=404'>405</a>\u001b[0m     \u001b[39m# wrap the array, to preserve subclasses\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/numpy/lib/shape_base.py?line=405'>406</a>\u001b[0m     buff \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39m__array_wrap__(buff)\n",
      "File \u001b[0;32m~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py:527\u001b[0m, in \u001b[0;36mTextSampler.__generate_sentence\u001b[0;34m(self, feature_mask)\u001b[0m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=524'>525</a>\u001b[0m \u001b[39mfor\u001b[39;00m word_idx \u001b[39min\u001b[39;00m masked_word:\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=525'>526</a>\u001b[0m     mod_sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(sentence_cp)\n\u001b[0;32m--> <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=526'>527</a>\u001b[0m     words, probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprob(mod_sentence)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=527'>528</a>\u001b[0m     sentence_cp[word_idx] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(words, p\u001b[39m=\u001b[39mprobs)\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=529'>530</a>\u001b[0m feature_mask \u001b[39m=\u001b[39m sentence_cp \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m|U80\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py:422\u001b[0m, in \u001b[0;36mTextSampler.prob\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=418'>419</a>\u001b[0m \u001b[39mif\u001b[39;00m sentence \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprob_cache:\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=419'>420</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprob_cache[sentence]\n\u001b[0;32m--> <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=421'>422</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpred_topk_cbow(sentence)\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=422'>423</a>\u001b[0m normalized_result \u001b[39m=\u001b[39m [(a, exp_normalize(b)) \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m result]\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=424'>425</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprob_cache[sentence] \u001b[39m=\u001b[39m normalized_result\n",
      "File \u001b[0;32m~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py:445\u001b[0m, in \u001b[0;36mTextSampler.pred_topk_cbow\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=441'>442</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([encoded_text])\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=443'>444</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=444'>445</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\u001b[39minput\u001b[39;49m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=446'>447</a>\u001b[0m \u001b[39m# get idx for mask token\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=447'>448</a>\u001b[0m mask_token_idx \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=448'>449</a>\u001b[0m     np\u001b[39m.\u001b[39marray(encoded_text) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmask_token_id\n\u001b[1;32m    <a href='file:///~/iml-ws21-projects-risingnumpygods/Anchor/sampler.py?line=449'>450</a>\u001b[0m )\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:629\u001b[0m, in \u001b[0;36mDistilBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=620'>621</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=621'>622</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=622'>623</a>\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=623'>624</a>\u001b[0m \u001b[39m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=624'>625</a>\u001b[0m \u001b[39m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=625'>626</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=626'>627</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=628'>629</a>\u001b[0m dlbrt_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=629'>630</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=630'>631</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=631'>632</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=632'>633</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=633'>634</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=634'>635</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=635'>636</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=636'>637</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=637'>638</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m dlbrt_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=638'>639</a>\u001b[0m prediction_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_transform(hidden_states)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:549\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=546'>547</a>\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=547'>548</a>\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=548'>549</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=549'>550</a>\u001b[0m     x\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=550'>551</a>\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=551'>552</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=552'>553</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=553'>554</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=554'>555</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=555'>556</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:327\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=323'>324</a>\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=324'>325</a>\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=326'>327</a>\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=327'>328</a>\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=328'>329</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=329'>330</a>\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=331'>332</a>\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:271\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=260'>261</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=261'>262</a>\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=262'>263</a>\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=267'>268</a>\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=268'>269</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=269'>270</a>\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=270'>271</a>\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=271'>272</a>\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=272'>273</a>\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=273'>274</a>\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=274'>275</a>\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=275'>276</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=276'>277</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=277'>278</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=278'>279</a>\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=279'>280</a>\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:202\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=199'>200</a>\u001b[0m q \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_lin(query))  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=200'>201</a>\u001b[0m k \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_lin(key))  \u001b[39m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=201'>202</a>\u001b[0m v \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_lin(value))  \u001b[39m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=203'>204</a>\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(dim_per_head)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py?line=204'>205</a>\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(q, k\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/iML/lib/python3.9/site-packages/torch/nn/functional.py?line=1847'>1848</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text_to_be_explained = \"This is a good book .\"\n",
    "\n",
    "preprocessed_text = [word.text for word in nlp(text_to_be_explained)]\n",
    "\n",
    "explainer = explainer = Anchor(Tasktype.TEXT)\n",
    "\n",
    "method_paras = {\"beam_size\": 1, \"desired_confidence\": 0.95}\n",
    "anchor = explainer.explain_instance(\n",
    "    preprocessed_text,\n",
    "    predict_lr,\n",
    "    method=\"beam\",\n",
    "    method_specific=method_paras,\n",
    "    num_coverage_samples=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mThis\u001b[0m is a \u001b[93mgood\u001b[0m \u001b[93mbook\u001b[0m .\n"
     ]
    }
   ],
   "source": [
    "visu = explainer.visualize(anchor, preprocessed_text)\n",
    "print(visu)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ee014a394539ea33a681f57bab852aba5460f711bf7a82fe5b61ba0c6bc8483"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('iML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
